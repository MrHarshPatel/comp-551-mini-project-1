{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_logistic_model_args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-efbf3d84b1a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \"LogisticRegression\": {\n\u001b[1;32m      3\u001b[0m         \u001b[0;34m\"data_headers\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;34m\"model_args\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbest_logistic_model_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     },\n\u001b[1;32m      6\u001b[0m     \"NaiveBayes\": {\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_logistic_model_args' is not defined"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"LogisticRegression\": {\n",
    "        \"data_headers\": False,\n",
    "        \"model_args\": best_logistic_model_args + [True]\n",
    "    },\n",
    "    \"NaiveBayes\": {\n",
    "        \"data_headers\": True,\n",
    "        \"model_args\":[False] \n",
    "    },\n",
    "    \"NaiveBayesLog\": {\n",
    "        \"data_headers\": True,\n",
    "        \"model_args\":[True] \n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(dataset, train_percentage):\n",
    "    dataset_copy = dataset\n",
    "    length = len(dataset_copy)\n",
    "    train_length = int(train_percentage * length)\n",
    "    \n",
    "    # Shuffle it!\n",
    "    np.random.shuffle(dataset_copy)\n",
    "    \n",
    "    train = dataset_copy[:train_length]\n",
    "    test = dataset_copy[train_length:]\n",
    "    return {\"train\": train, \"test\": test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_datasets(train_percentage):\n",
    "    splits = {}\n",
    "    print(\"Splitting with training {}% for each dataset -----\".format(train_percentage*100))\n",
    "    for key, dataset_arr in train_val.items():\n",
    "        split = get_split(dataset_arr[0],train_percentage)\n",
    "        \n",
    "        print(\"\\t {} Train Size: {}\".format(key,len(split[\"train\"])))\n",
    "        print(\"\\t {} Test Size: {}\".format(key,len(test_val_new[key])))\n",
    "        splits[key] = split\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tests(train_percentages):\n",
    "    for train_percentage in train_percentages:\n",
    "        splits = split_datasets(train_percentage)\n",
    "        for dataset_name, dataset in splits.items():\n",
    "            for model_name, model_params in models.items():\n",
    "                print()\n",
    "                print(\"\\tFitting {} with {} \".format(dataset_name,model_name))\n",
    "                \n",
    "                if(model_params[\"data_headers\"] == True):\n",
    "                    model = run_fit(splits[dataset_name][\"train\"],model_name, train_val[dataset_name][1],[] ,model_params[\"model_args\"])\n",
    "                else:\n",
    "                    model = run_fit(splits[dataset_name][\"train\"],model_name,[],test_val_new[dataset_name], model_params[\"model_args\"], dataset_name)\n",
    "\n",
    "                print(\"\\tPredicting {} with {}\".format(dataset_name, model_name))\n",
    "                print(\"\\t\\tGot accuracy of: {}\".format(run_predict(test_val_new[dataset_name],model)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'split_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b1f00e334d3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Splits of 30%, 50%, 70% and 100% for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msplit_tests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-c4995a80ebe4>\u001b[0m in \u001b[0;36msplit_tests\u001b[0;34m(train_percentages)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msplit_tests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_percentages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrain_percentage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_percentages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0msplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_percentage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_params\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'split_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "# Splits of 30%, 50%, 70% and 100% for training\n",
    "split_tests([0.1,0.3,0.5,0.7,1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
